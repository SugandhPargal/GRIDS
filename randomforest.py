# -*- coding: utf-8 -*-
"""RandomForest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dzy5YxLIOXJPWQZ_YkiK97W9wYqqKaqX
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import numpy as np
from sklearn import tree
data = pd.read_csv('D1_Speed_Profile.csv')
df = pd.DataFrame(data, columns = data.columns)

df.shape
df = df.sample(frac = 1)

x = df.drop(columns = ["Filename","Rating"])
#print(x[0:5])
y = df["Rating"]#save the feature name and target variables
feature_names = x.columns

labels = y.unique()#split the dataset
le = LabelEncoder()
le.fit(y.astype(str))
y = le.transform(y.astype(str))

X_train, test_x, y_train, test_lab = train_test_split(x,y,
                                                 test_size = 0.3,
                                                 random_state = 42)
unique, counts = np.unique(y_train, return_counts=True)
#dict(zip(unique, counts))

#Create a RF Classifier
clf=RandomForestClassifier(n_estimators=10,
                            max_depth=3,
                            max_features='auto',
                            bootstrap=True,
                            n_jobs=-1,
                            random_state=0)

#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(X_train,y_train)
spare_cn=str(labels)
test_labels= str(y)
print(spare_cn)
print(np.unique(y))

train_pred = clf.predict(X_train)
feature_imp = pd.Series(clf.feature_importances_,index=feature_names).sort_values(ascending=False)
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.rcParams["figure.figsize"] = (7,10)
plt.legend()
plt.show()

for i,v in enumerate(feature_imp):
	print('Feature: %s, Score: %.5f' % (feature_imp.index[i],v))

!pip install shap
import shap

X_importance = X_train ######## test set passed, can be passed training set as well

# Explain model predictions using shap library:
explainer = shap.TreeExplainer(clf) ###### lgb_model is the fitted model
shap_values = explainer.shap_values(X_importance)
shap.summary_plot(shap_values, X_importance)

importance_df = pd.DataFrame(index=[X_importance.columns.tolist()])
#importance_df = pd.DataFrame([X_importance.columns.tolist()]).T
#importance_df.columns = ['column_name']
print(importance_df)
for i in np.unique(y):
  shap_sum = np.abs(shap_values[i]).mean(axis=0)
  shap_sum= shap_sum.astype('float')
  importance_df[i] = shap_sum.tolist()



importance_df = importance_df.sort_values(0, ascending=False)
result = importance_df.idxmax(axis=1)
importance_df
importance_df.head(5)
print(result)
data_top = importance_df.head(5)
important_fetures= data_top.index.tolist()

shap_sum = np.abs(shap_values[0]).mean(axis=0)
importance_df = pd.DataFrame([X_importance.columns.tolist(), shap_sum.tolist()]).T
importance_df.columns = ['column_name', 'shap_importance']
importance_df = importance_df.sort_values('shap_importance', ascending=False)
importance_df

# -*- coding: utf-8 -*-
"""
Created on Thu Jan 28 18:51:26 2021

@author: 91987
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import numpy as np
from sklearn import tree
data = pd.read_csv('D1_Speed_Profile.csv')
df = pd.DataFrame(data, columns = data.columns)

df.shape
df = df.sample(frac = 1)
print(df[0:10])

df.isnull().values.any()

#x = df.drop(columns=["Label"] )
#roadtype = df['scene']
#labels = roadtype.unique()
#le = LabelEncoder()
#le.fit(roadtype.astype(str))
#roadtype = le.transform(roadtype.astype(str))
#df['roadtype'] = roadtype

#x = df.drop(columns=['FileName','Timestamp', 'City','Ground Truth Rating'] )

x = df.drop(columns = ["Filename","Rating"])
print(x[0:5])
y = df["Rating"]#save the feature name and target variables
feature_names = x.columns

labels = y.unique()#split the dataset
le = LabelEncoder()
le.fit(y.astype(str))
y = le.transform(y.astype(str))

X_train, test_x, y_train, test_lab = train_test_split(x,y,
                                                 test_size = 0.3,
                                                 random_state = 42)
unique, counts = np.unique(y_train, return_counts=True)
dict(zip(unique, counts))
X_train

#Create a Gaussian Classifier
clf=RandomForestClassifier(n_estimators=10,
                            max_depth=3,
                            max_features='auto',
                            bootstrap=True,
                            n_jobs=-1,
                            random_state=0)

#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(X_train,y_train)

# a = tree.plot_tree(clf,
#                    #use the feature names stored
#                    feature_names = feature_names,
#                    #use the class names stored
#                    class_names = str(labels),
#                    rounded = True,
#                    filled = True,
#                    fontsize=14)#show the plot
# plt.show()

from sklearn.tree import _tree
def get_rules(tree, feature_names, class_names):

    tree_ = tree.tree_
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"
        for i in tree_.feature
    ]

    paths = []
    path = []

    def recurse(node, path, paths):

        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree_.threshold[node]
            p1, p2 = list(path), list(path)
            p1 += [f"({name} <= {np.round(threshold, 3)})"]
            recurse(tree_.children_left[node], p1, paths)
            p2 += [f"({name} > {np.round(threshold, 3)})"]
            recurse(tree_.children_right[node], p2, paths)
        else:
            path += [(tree_.value[node], tree_.n_node_samples[node])]
            paths += [path]

    recurse(0, path, paths)

    # sort by samples count
    samples_count = [p[-1][1] for p in paths]
    ii = list(np.argsort(samples_count))
    paths = [paths[i] for i in reversed(ii)]

    rules = []
    for path in paths:
        rule = "if "

        for p in path[:-1]:
            if rule != "if ":
                rule += " and "
            rule += str(p)
        rule += " then "
        if class_names is None:
            rule += "response: "+str(np.round(path[-1][0][0][0],3))
        else:
            classes = path[-1][0][0]
            l = np.argmax(classes)
            rule += "class:" + str(class_names[l])
        rule += f" | based on {path[-1][1]:,} samples"
        rules += [rule]

    return rules

spare_cn=str(labels)
print(spare_cn)

from sklearn.tree import plot_tree
f1 = open('Newyork_Maneuvers_rules.txt', 'a')
fn=feature_names
spare_cn=str(labels)
cn=['3','4']

for tree_idx, est in enumerate(clf.estimators_):
    f1.write('TREE: {}'.format(tree_idx) + '\n')
    print('TREE:'.format(tree_idx) + '\n')
    rules = get_rules(est, fn, cn)
    for r in rules:
        f1.write('{}'.format(r) + '\n')
        print(r)

y_pred=clf.predict(test_x)
y_train_pred = clf.predict(X_train)
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(test_lab, y_pred))
print("Accuracy of training:",metrics.accuracy_score(y_train, y_train_pred))
test_lab = list(test_lab)
for i in range(len(test_lab)):
    print(test_lab[i], ' ', y_pred[i])

train_pred = clf.predict(X_train)
feature_imp = pd.Series(clf.feature_importances_,index=feature_names).sort_values(ascending=False)
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.rcParams["figure.figsize"] = (7,10)
plt.legend()
plt.show()

sum_feature= sum(feature_imp)

print(sum_feature)

for i,v in enumerate(feature_imp):
	print('Feature: %s, Score: %.5f' % (feature_imp.index[i],v))

train_pred = clf.predict(X_train)
feature_imp = pd.Series(clf.feature_importances_,index=feature_names).sort_values(ascending=False)
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()


confusion_matrix = metrics.confusion_matrix(test_lab,
                                            y_pred)#turn this into a dataframe
matrix_df = pd.DataFrame(confusion_matrix)#plot the result
ax = plt.axes()
sns.set(font_scale=1.3)
plt.figure(figsize=(10,7))
sns.heatmap(matrix_df, annot=True, fmt="g", ax=ax, cmap="magma")#set axis titles
ax.set_title('Confusion Matrix - Random Forest')
ax.set_xlabel("Predicted label", fontsize =15)
ax.set_xticklabels(labels)
ax.set_ylabel("True Label", fontsize=15)
ax.set_yticklabels(list(labels), rotation = 0)
plt.show()



print(metrics.classification_report(test_lab,
                                    y_pred))
#accuracy of training
print("Accuracy of training")
print(metrics.classification_report(y_train,
                                    train_pred))

pip install shap

import shap

X_importance = test_x ######## test set passed, can be passed training set as well

# Explain model predictions using shap library:
explainer = shap.TreeExplainer(clf) ###### lgb_model is the fitted model
shap_values = explainer.shap_values(X_importance)
shap.summary_plot(shap_values, X_importance)

mzzz= max(shap_values)

sv_cls1 = shap_values[0]

sv_cls2 = shap_values[1]

print(sv_cls)

vals1=np.abs(sv_cls1).mean(0)
vals2=np.abs(sv_cls2).mean(0)

shap_importance1 = pd.DataFrame(list(zip(feature_names, vals1)), columns=['col_name', 'feature_importance_vals'])
shap_importance1.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)

shap_importance2 = pd.DataFrame(list(zip(feature_names, vals2)), columns=['col_name', 'feature_importance_vals'])
shap_importance2.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)

print(shap_importance1)
print(shap_importance2)

print(shap_values)

f1 = open('JerseyCity_rules.txt', 'a')

for tree_idx, est in enumerate(clf.estimators_):
    f1.write('TREE: {}'.format(tree_idx) + '\n')
    text_representation = tree.export_text(est)
    f1.write('{}'.format(text_representation) + '\n')
    print(text_representation)

cn=['0','1','2','3']

cn

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
fn=feature_names
cn=labels
fig, axes = plt.subplots(nrows = 1,ncols = 10,figsize = (20,2), dpi=900)
for index in range(0, 10):
    tree.plot_tree(clf.estimators_[index],
                   feature_names = fn,
                   class_names=str(labels),
                   filled = True,
                   ax = axes[index]);

    axes[index].set_title('Estimator: ' + str(index), fontsize = 11)

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
fn=feature_names

fig = plt.figure(figsize=(15, 10))

plot_tree(clf.estimators_[0],
          feature_names = fn,
          class_names=str(labels),
          filled=True, impurity=True,
          rounded=True)

text_representation = tree.export_text(clf)
print(text_representation)

def print_decision_rules(rf):

    for tree_idx, est in enumerate(rf.estimators_):
        tree = est.tree_
        assert tree.value.shape[1] == 1 # no support for multi-output

        print('TREE: {}'.format(tree_idx))
        f1.write('TREE: {}'.format(tree_idx) + '\n')

        iterator = enumerate(zip(tree.children_left, tree.children_right, tree.feature, tree.threshold, tree.value))
        for node_idx, data in iterator:
            left, right, feature, th, value = data

            # left: index of left child (if any)
            # right: index of right child (if any)
            # feature: index of the feature to check
            # th: the threshold to compare against
            # value: values associated with classes

            # for classifier, value is 0 except the index of the class to return
            class_idx = np.argmax(value[0])

            if left == -1 and right == -1:
                print('{} LEAF: return class={}'.format(node_idx, class_idx))
                f1.write('{} LEAF: return class={}'.format(node_idx, class_idx) + '\n')
            else:
                print('{} NODE: if feature[{}] < {} then next={} else next={}'.format(node_idx, feature, th, left, right))
                f1.write('{} NODE: if feature[{}] < {} then next={} else next={}'.format(node_idx, feature, th, left, right) + '\n')

print_decision_rules(clf)
f1 = open('C:\\Users\\91987\\Desktop\\semi-personalized-datasets\\dataset_with_new_annotations\\group2_rules.txt', 'a')
y_pred=clf.predict(test_x)
y_train_pred = clf.predict(X_train)
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(test_lab, y_pred))
print("Accuracy of training:",metrics.accuracy_score(y_train, y_train_pred))
test_lab = list(test_lab)
for i in range(len(test_lab)):
    print(test_lab[i], ' ', y_pred[i])

#prediction for each instances
clf.predict([[1,1,1]])



train_pred = clf.predict(X_train)
feature_imp = pd.Series(clf.feature_importances_,index=feature_names).sort_values(ascending=False)
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()


confusion_matrix = metrics.confusion_matrix(test_lab,
                                            y_pred)#turn this into a dataframe
matrix_df = pd.DataFrame(confusion_matrix)#plot the result
ax = plt.axes()
sns.set(font_scale=1.3)
plt.figure(figsize=(10,7))
sns.heatmap(matrix_df, annot=True, fmt="g", ax=ax, cmap="magma")#set axis titles
ax.set_title('Confusion Matrix - Random Forest')
ax.set_xlabel("Predicted label", fontsize =15)
ax.set_xticklabels(['']+labels)
ax.set_ylabel("True Label", fontsize=15)
ax.set_yticklabels(list(labels), rotation = 0)
plt.show()



print(metrics.classification_report(test_lab,
                                    y_pred))
#accuracy of training
print("Accuracy of training")
print(metrics.classification_report(y_train,
                                    train_pred))
############### cross - validation ###############################
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
#kf5 = KFold(n_splits=5, shuffle=False)
cv = KFold(n_splits=3, random_state=1, shuffle=True)
# create model

# evaluate model
scores = cross_val_score(clf, x, y, scoring='accuracy', cv=cv, n_jobs=-1)
# report performance
print(scores)

########################################## training with all features ################################

data = pd.read_csv('C:\\Users\\91987\\Desktop\\semi-personalized-datasets\\dataset_with_new_annotations\\Final_Rating_Sheet - whole_group.csv')
df = pd.DataFrame(data, columns = data.columns)
df.shape
df = df.sample(frac = 1)
print(df[0:10])
#x = df.drop(columns=["Label"] )
#roadtype = df['scene']
#labels = roadtype.unique()
#le = LabelEncoder()
#le.fit(roadtype.astype(str))
#roadtype = le.transform(roadtype.astype(str))
#df['roadtype'] = roadtype
x = df.drop(columns=['FileName', 'Timestamp', 'Jerkiness', 'Ground Truth Rating', 'City'] )

#x = df.drop(columns = "Front-Brake")
print(x[0:5])
y = df["Ground Truth Rating"]#save the feature name and target variables
feature_names = x.columns
labels = y.unique()#split the dataset
le = LabelEncoder()
le.fit(y.astype(str))
y = le.transform(y.astype(str))

X_train, test_x, y_train, test_lab = train_test_split(x,y,
                                                 test_size = 0.4,
                                                 random_state = 42)
unique, counts = np.unique(y_train, return_counts=True)
dict(zip(unique, counts))
#Create a Gaussian Classifier
clf_whole=RandomForestClassifier(n_estimators=100)

#Train the model using the training sets y_pred=clf.predict(X_test)
clf_whole.fit(X_train,y_train)

# a = tree.plot_tree(clf,
#                    #use the feature names stored
#                    feature_names = feature_names,
#                    #use the class names stored
#                    class_names = str(labels),
#                    rounded = True,
#                    filled = True,
#                    fontsize=14)#show the plot
# plt.show()

def print_decision_rules(rf):

    for tree_idx, est in enumerate(rf.estimators_):
        tree = est.tree_
        assert tree.value.shape[1] == 1 # no support for multi-output

        print('TREE: {}'.format(tree_idx))
        f1.write('TREE: {}'.format(tree_idx) + '\n')

        iterator = enumerate(zip(tree.children_left, tree.children_right, tree.feature, tree.threshold, tree.value))
        for node_idx, data in iterator:
            left, right, feature, th, value = data

            # left: index of left child (if any)
            # right: index of right child (if any)
            # feature: index of the feature to check
            # th: the threshold to compare against
            # value: values associated with classes

            # for classifier, value is 0 except the index of the class to return
            class_idx = np.argmax(value[0])

            if left == -1 and right == -1:
                print('{} LEAF: return class={}'.format(node_idx, class_idx))
                f1.write('{} LEAF: return class={}'.format(node_idx, class_idx) + '\n')
            else:
                print('{} NODE: if feature[{}] < {} then next={} else next={}'.format(node_idx, feature, th, left, right))
                f1.write('{} NODE: if feature[{}] < {} then next={} else next={}'.format(node_idx, feature, th, left, right) + '\n')

print_decision_rules(clf_whole)
f1 = open('C:\\Users\\91987\\Desktop\\semi-personalized-datasets\\dataset_with_new_annotations\\group2_rules.txt', 'a')
y_pred=clf_whole.predict(test_x)
y_train_pred = clf_whole.predict(X_train)
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(test_lab, y_pred))
print("Accuracy of training:",metrics.accuracy_score(y_train, y_train_pred))
test_lab = list(test_lab)
for i in range(len(test_lab)):
    print(test_lab[i], ' ', y_pred[i])



#prediction for each instances
#output = clf_whole.predict([[1,1,1,1,1,1]])
################## Testing dataset after bootstrapping ####################
#X_valid, test_x, y_valid, test_lab = train_test_split(test_x,test_lab,
#                                                 test_size = 0.5,
#                                                 random_state = 42)
################ Reading in-house data ####################################

data1 = pd.read_csv('C:\\Users\\91987\\Desktop\\semi-personalized-datasets\\dataset_with_new_annotations\\in-house-dataset.csv')
df1 = pd.DataFrame(data1, columns = data1.columns)
df1.shape
df1 = df1.sample(frac = 1)
print(df1[0:10])
x = df1.drop(columns=['FileName', 'Timestamp', 'Ground Truth Rating', 'City'] )

#x = df.drop(columns = "Front-Brake")
print(x[0:5])
y = df1["Ground Truth Rating"]#save the feature name and target variables
feature_names = x.columns
# labels = y.unique()#split the dataset
# le = LabelEncoder()
# le.fit(y.astype(str))
# y = le.transform(y.astype(str))
test_x = test_x.append(x)
test_lab = test_lab.append(y)

unique, counts = np.unique(test_lab, return_counts=True)
dict(zip(unique, counts))









len(test_lab), len(test_x)
in_vehicle_features = test_x.drop(['RelSpeed', 'RelDist', 'BrakeLight'], axis =1)
count = 0
count2 = 0
data_to_feed_X = pd.DataFrame()
#columns=['Jerkiness feature value',  'RelSpeed',  'RelDist','BrakeLight',  'Turns',  'Stops'] )
data_to_feed_Y = pd.DataFrame()
Train_Acc = []
Test_Acc = []
#test_x.drop(test_x.iloc[0], axis=0)
test_x = test_x.reset_index(drop=True)
test_lab = test_lab.reset_index(drop = True)
#test_x.drop([0])
y_test_pred=clf_whole.predict(test_x)
for i in range(len(y_test_pred)):
    if(y_test_pred[i] != test_lab[i]):
        count = count + 1
        in_vehicle_output = clf.predict(in_vehicle_features[i:i+1])
        #print(in_vehicle_features[i:i+1], in_vehicle_output[0])
        if(y_test_pred[i]>= in_vehicle_output[0]):

            count2  = count2 + 1
            data_to_feed_X = data_to_feed_X.append(test_x[i: i+1])
            #data_to_feed_Y.append(test_lab[i])
            data_to_feed_Y = data_to_feed_Y.append(test_lab[i: i+1])
            X_train = X_train.append(test_x[i: i+1])
            #y_train = np.append(y_train, test_lab[i])
            y_train = y_train.append(test_lab[i: i+1])
            clf_whole.fit(X_train,y_train)

            test_x = test_x.drop([i], axis=0)
            test_lab = test_lab.drop([i], axis=0)
            #test_lab = np.delete(test_lab, i)
            y_pred=clf_whole.predict(test_x)
            y_train_pred = clf_whole.predict(X_train)

            print("Accuracy:",metrics.accuracy_score(test_lab, y_pred))

            print("Accuracy of training:",metrics.accuracy_score(y_train, y_train_pred))
            Train_Acc.append(metrics.accuracy_score(y_train, y_train_pred))
            Test_Acc.append(metrics.accuracy_score(test_lab, y_pred))


clf_whole.fit(X_train,y_train)
y_pred=clf_whole.predict(test_x)
y_train_pred = clf_whole.predict(X_train)
print("Accuracy:",metrics.accuracy_score(test_lab, y_pred))
Train_Acc = []
Test_Acc = []
print("Accuracy of training:",metrics.accuracy_score(y_train, y_train_pred))
for i in range(0,49):
    X_train = X_train.append(data_to_feed_X[i: i+1])
    y_train = np.append(y_train, data_to_feed_Y[i])
    clf_whole.fit(X_train,y_train)
    y_pred=clf_whole.predict(test_x)
    y_train_pred = clf_whole.predict(X_train)
    print("Accuracy:",metrics.accuracy_score(test_lab, y_pred))

    print("Accuracy of training:",metrics.accuracy_score(y_train, y_train_pred))
    Train_Acc.append(metrics.accuracy_score(y_train, y_train_pred))
    Test_Acc.append(metrics.accuracy_score(test_lab, y_pred))





plt.plot(Train_Acc)
plt.plot(Test_Acc)
#plt.plot(temp1)
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['trained on BDD', 'tested on in-house + 40% BDD'], loc='best')
plt.show()

temp1 = Test_Acc
temp2 = Train_Acc



train_pred = clf_whole.predict(X_train)
feature_imp = pd.Series(clf_whole.feature_importances_,index=feature_names).sort_values(ascending=False)
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()


confusion_matrix = metrics.confusion_matrix(test_lab,
                                            y_pred)#turn this into a dataframe
matrix_df = pd.DataFrame(confusion_matrix)#plot the result
ax = plt.axes()
sns.set(font_scale=1.3)
plt.figure(figsize=(10,7))
sns.heatmap(matrix_df, annot=True, fmt="g", ax=ax, cmap="magma")#set axis titles
ax.set_title('Confusion Matrix - Random Forest')
ax.set_xlabel("Predicted label", fontsize =15)
ax.set_xticklabels(['']+labels)
ax.set_ylabel("True Label", fontsize=15)
ax.set_yticklabels(list(labels), rotation = 0)
plt.show()



print(metrics.classification_report(test_lab,
                                    y_pred))
#accuracy of training
print("Accuracy of training")
print(metrics.classification_report(y_train,
                                    train_pred))
############### cross - validation ###############################
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
#kf5 = KFold(n_splits=5, shuffle=False)
cv = KFold(n_splits=3, random_state=1, shuffle=True)
# create model

# evaluate model
scores = cross_val_score(clf, x, y, scoring='accuracy', cv=cv, n_jobs=-1)
# report performance
print(scores)














random_forest.py
Displaying random_forest.py.

import nltk
nltk.download('punkt')
filex = open('Newyork_rules.txt', 'r')
text = filex.read()
sentences1= text.split('\n')
#print(sentences1)

sentences1= sentences1[0:-1]

print(sentences1[1])

sentences= [nltk.word_tokenize(sent) for sent in sentences1]
#tokens = nltk.sent_tokenize(text)
#nltk.word_tokenize(sent)
print(sentences)

from gensim.models import Word2Vec

from nltk.cluster import KMeansClusterer
import nltk
import numpy as np

from sklearn import cluster
from sklearn import metrics

# training data

"""sentences = [['this', 'is', 'the', 'one','good', 'machine', 'learning', 'book'],
            ['this', 'is',  'another', 'book'],
            ['one', 'more', 'book'],
            ['weather', 'rain', 'snow'],
            ['yesterday', 'weather', 'snow'],
            ['forecast', 'tomorrow', 'rain', 'snow'],
            ['this', 'is', 'the', 'new', 'post'],
            ['this', 'is', 'about', 'more', 'machine', 'learning', 'post'],
            ['and', 'this', 'is', 'the', 'one', 'last', 'post', 'book']]"""



model = Word2Vec(sentences, min_count=1)


def sent_vectorizer(sent, model):
    sent_vec =[]
    numw = 0
    for w in sent:
        try:
            if numw == 0:
                sent_vec = model[w]
            else:
                sent_vec = np.add(sent_vec, model[w])
            numw+=1
        except:
            pass

    return np.asarray(sent_vec) / numw


X=[]
for sentence in sentences:
    X.append(sent_vectorizer(sentence, model))

print ("========================")
print (X)




# note with some version you would need use this (without wv)
#  model[model.vocab]
print (model[model.wv.vocab])




#print (model.similarity('post', 'book'))
#print (model.most_similar(positive=['machine'], negative=[], topn=2))





NUM_CLUSTERS=3
kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)
assigned_clusters = kclusterer.cluster(X, assign_clusters=True)
print (assigned_clusters)



for index, sentence in enumerate(sentences):
    print (str(assigned_clusters[index]) + ":" + str(sentence))




kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)
kmeans.fit(X)

labels = kmeans.labels_
centroids = kmeans.cluster_centers_

print ("Cluster id labels for inputted data")
print (labels)
print ("Centroids data")
print (centroids)

print ("Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):")
print (kmeans.score(X))

silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')

print ("Silhouette_score: ")
print (silhouette_score)


import matplotlib.pyplot as plt

from sklearn.manifold import TSNE

model = TSNE(n_components=2, random_state=0)
np.set_printoptions(suppress=True)

Y=model.fit_transform(X)


plt.scatter(Y[:, 0], Y[:, 1], c=assigned_clusters, s=290,alpha=.5)


for j in range(len(sentences)):
   plt.annotate(assigned_clusters[j],xy=(Y[j][0], Y[j][1]),xytext=(0,0),textcoords='offset points')
   print ("%s %s" % (assigned_clusters[j],  sentences[j]))


plt.show()

"""TF-idf vectorizer to infer similarities between each pair of sentences and create 2d NxN matrix"""

import numpy as np
from scipy import spatial

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('wordnet')
# Download stopwords list
nltk.download('punkt')
stop_words = set(stopwords.words('english'))

# Interface lemma tokenizer from nltk with sklearn
class LemmaTokenizer:
    ignore_tokens = [',', '.', ';', ':', '"', '``', "''", '`']
    def __init__(self):
        self.wnl = WordNetLemmatizer()
    def __call__(self, doc):
        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]

fp = open('JerseyCity_inter.txt', 'r')
lst = []
for line in fp:
  line = line[:-1]
  lst.append(line)

print(lst, len(lst))

from re import search
# Lemmatize the stop words
tokenizer=LemmaTokenizer()
token_stop = tokenizer(' '.join(stop_words))
list2 = []
scores = [] #hold the 2-d matrix
for i in range(len(lst)):
  list2 = lst
  #list2.pop(i)
  search_terms = lst[i]
  documents = list2
  #print(len(documents), len(search_terms))
  #print(search_terms, type(search_terms), type(documents))
  # Create TF-idf model
  vectorizer = TfidfVectorizer(stop_words=token_stop,
                                tokenizer=tokenizer)
  doc_vectors = vectorizer.fit_transform([search_terms] + documents)
  # Calculate similarity
  cosine_similarities = linear_kernel(doc_vectors[0:1], doc_vectors).flatten()
  document_scores = [item.item() for item in cosine_similarities[1:]]
  # [0.0, 0.287]
  #print(document_scores)
  print(len(document_scores))
  scores.append(document_scores)

len(scores)
print(scores)
scores = np.array(scores)
#scores

templist = []
count = 0
i = 0
dict1 = dict()
for list1 in scores:
  templist = list1
  count = 0
  for j in list1:
    if(j>0.78):
      count = count + 1
  dict1[i] = count
  i = i + 1

sort_orders = sorted(dict1.items(), key=lambda x: x[1], reverse=True)
count = 0
for i in sort_orders:
	print(lst[i[0]] , 'with the frequency', i[1])

"""Using soft-cosine similarity"""

from re import sub
from gensim.utils import simple_preprocess

query_string = 'Self vehicle takes abrupt turn and Self vehicle takes smooth stop.'
documents = ['Self vehicle takes abrupts stop and Front vehicle takes a brake.', 'Relative distance between two vehicles is high and Front vehicle doesnt brake and Self vehicle takes abrupt turn.']

stopwords = ['the', 'and', 'are', 'a', 'takes']

# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb
def preprocess(doc):
    # Tokenize, clean up input document string
    doc = sub(r'<img[^<>]+(>|$)', " image_token ", doc)
    doc = sub(r'<[^<>]+(>|$)', " ", doc)
    doc = sub(r'\[img_assist[^]]*?\]', " ", doc)
    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', " url_token ", doc)
    return [token for token in simple_preprocess(doc, min_len=0, max_len=float("inf")) if token not in stopwords]

# Preprocess the documents, including the query string
corpus = [preprocess(document) for document in documents]
query = preprocess(query_string)

print(query)

pip install --upgrade gensim

import gensim.downloader as api
from gensim.corpora import Dictionary
from gensim.models import TfidfModel
from gensim.similarities  import WordEmbeddingSimilarityIndex
from gensim.similarities import SparseTermSimilarityMatrix
from gensim.similarities import SoftCosineSimilarity
from nltk import corpus
# Load the model: this is a big file, can take a while to download and open
glove = api.load("glove-wiki-gigaword-50")
similarity_index = WordEmbeddingSimilarityIndex(glove)

# Build the term dictionary, TF-idf model
print(corpus)

dictionary = Dictionary(corpus+[query])

tfidf = TfidfModel(dictionary=dictionary)
# Create the term similarity matrix.
similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)

print(similarity_matrix)

import numpy as np
# Compute Soft Cosine Measure between the query and the documents.
# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb
query_tf = tfidf[dictionary.doc2bow(query)]

index = SoftCosineSimilarity(
            tfidf[[dictionary.doc2bow(document) for document in corpus]],
            similarity_matrix)

doc_similarity_scores = index[query_tf]

# Output the sorted similarity scores and documents
sorted_indexes = np.argsort(doc_similarity_scores)[::-1]
print(doc_similarity_scores)
for idx in sorted_indexes:
   print(f'{idx} \t {doc_similarity_scores[idx]:0.3f} \t {documents[idx]}')

# 1    0.688    tomatoes are actually fruit
# 0    0.000    cars drive on the road